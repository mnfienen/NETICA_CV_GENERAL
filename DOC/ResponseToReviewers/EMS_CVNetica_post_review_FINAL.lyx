#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass elsarticle
\begin_preamble
\usepackage{lineno}
\renewcommand\[{\begin{equation}}
\renewcommand\]{\end{equation}}
\bibpunct{(}{)}{,}{a}{,}{,}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing double
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes true
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
A cross-validation package driving Netica with Python 
\end_layout

\begin_layout Author
Michael N.
 Fienen
\end_layout

\begin_layout Address
US Geological Survey, Wisconsin Water Science Center, 8505 Research Way,
 Middleton WI 53562 USA
\end_layout

\begin_layout Address
+001 (608) 821-3894
\end_layout

\begin_layout Email
mnfienen@usgs.gov
\end_layout

\begin_layout Author
Nathaniel G.
 Plant
\end_layout

\begin_layout Address
US Geological Survey, St.
 Petersburg Coastal and Marine Science Center, 600 Fourth Street South,
 St.
 Petersburg, Florida, 33701, USA.
\end_layout

\begin_layout Email
nplant@usgs.gov
\end_layout

\begin_layout Keywords
Cross-validation; Bayesian networks; uncertainty; probability; Python; Netica;
 prediction
\end_layout

\begin_layout Abstract
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset

Bayesian networks (BNs) are powerful tools for probabilistically simulating
 natural systems and emulating process models.
 Cross validation is an important technique to avoid overfitting that can
 result from overly complex BNs.
 Overfitting results in a reduction of true predictive skill.
 Formal cross-validation for BNs has been discussed but rarely implemented.
 The lack of widespread cross-validation is due partly to a lack of software
 tools designed to work with available BN packages.
 CVNetica is an open-source package written in Python that extends the Netica
 software package to perform cross-validation and to read, rebuild, and
 learn BNs from data.
 Insights gained from cross-validation and implications on predictive versus
 descriptive skill are illustrated with two examples: a data-driven oceanographi
c application; and a model-emulation application.
 These examples show that overfitting does in fact occur when BNs become
 more complex than is allowed by supporting data and overfitting incurs
 computational costs as well as causing a reduction in prediction skill.
 CVNetica can be used to evaluate the degree of overfitting that results
 from a wide variety of complexity metrics (we used level of discretization)
 and its impact on a variety of performance metrics (we used skill).
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
maketitle
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Over the past two decades, the use of Bayesian networks 
\begin_inset CommandInset citation
LatexCommand citep
before "BN; "
key "JensenNielsen2001"

\end_inset

 has increased greatly, in large measure due to the availability of commercial
 software packages such as Netica 
\begin_inset CommandInset citation
LatexCommand citep
key "Netica"

\end_inset

 and Hugin 
\begin_inset CommandInset citation
LatexCommand citep
key "Hugin"

\end_inset

 among many others.
 Applications in water resources have included groundwater management 
\begin_inset CommandInset citation
LatexCommand citep
key "mdso2007,Molina2010,Molina2013"

\end_inset

, and model emulation 
\begin_inset CommandInset citation
LatexCommand citep
key "plant2011a,plant2011b,FienenBNWRR"

\end_inset

.
 This builds on a history of applications in national security, economics,
 and ecology
\end_layout

\begin_layout Standard
An important topic that is not always discussed in the literature is that
 applications of BNs should have formal tests and validation of prediction
 performance 
\begin_inset CommandInset citation
LatexCommand citep
key "Chen2012,Marcot2012"

\end_inset

.
 BNs, as described in detail below, are made up of nodes representing variables.
 These nodes are discretized into bins and connected by edges.
 The arrangement of nodes, edges, and bins all impact the ability of a BN
 to fit a dataset descriptively and to predict new data accurately.
 There is always a tradeoff---better descriptive ability often is attained
 at the expense of predictive power.
 When descriptive ability is too high, the BN is fitting noise rather than
 true variability and this leads to the well-known result of overfitting.
 To protect against this, it is important to evaluate both descriptive ability
 and predictive power.
\end_layout

\begin_layout Standard
Some validation metrics are calculable by the commercial software packages,
 but substantial gaps in capabilities remain.
 Fortunately---at least in the case of Netica---an application programming
 interface (API) exists with versions in multiple programming languages.
 To create a toolbox of performance metrics, we used Python 
\begin_inset CommandInset citation
LatexCommand citep
key "python"

\end_inset

 with the Netica C APIs.
 These APIs expose most of Netica's functionality, through functions, to
 external programming.
 Among the languages available, C was chosen because one of our goals was
 to interface with Python 2.7.6 
\begin_inset CommandInset citation
LatexCommand citep
key "python"

\end_inset

, Numpy 1.8 and Scipy 0.13.2 
\begin_inset CommandInset citation
LatexCommand citep
key "scipy"

\end_inset

 and Python can access C libraries and functions.
 We discuss the technical challenges associated with running C APIs using
 Python and describe the toolbox of validation metrics included in this
 work.
\end_layout

\begin_layout Standard
There are many methods and metrics available to evaluating model performance
 
\begin_inset CommandInset citation
LatexCommand citep
key "EMSPosition2013"

\end_inset

.
 The choices made in this work provide a starting point within a framework
 that other users can easily extend.
 Building on techniques introduced by 
\begin_inset CommandInset citation
LatexCommand citet
key "FienenBNWRR"

\end_inset

, we developed tools addressing two fundamental questions of Bayesian network
 performance: how does predictive performance compare with descriptive calibrati
on quality?; and how does the complexity of the underlying network impact
 predictive and descriptive performance? Cross-validation is used to answer
 both questions, and the number of bins per node is used as a metric of
 complexity to answer the second.
 The number of nodes and the nature of their connections through edges are
 other aspects of BN design that define the complexity of a BN and impact
 both descriptive and predictive power.
 Our framework allows for consideration and analysis of other validation
 metrics and techniques beyond those presented here.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Bayesian Networks
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This background section on Bayesian networks (BNs) is derived from 
\begin_inset CommandInset citation
LatexCommand citet
key "FienenBNWRR"

\end_inset

.
 A Bayesian network is a directed acyclic graph 
\begin_inset CommandInset citation
LatexCommand citep
key "Korb2004"

\end_inset

, composed of nodes and edges.
 Nodes represent variables whose parameter values may include Boolean, discrete
 states, or, for continuous variables, discrete ranges that are discretized
 into bins.
 Edges form the connections between nodes and represent a correlated connection
 between the properties represented by the nodes.
 The entire catalog of these correlations make up conditional probability
 tables (CPTs).
 In a predictive context, nodes can be thought of as either input (e.g.
 forcing) or output (e.g.
 response), although this distinction is not a sharp one as the conditional
 probabilities learned by the BN are ambivalent with respect to direction.
 Nodes can also be intermediate if they act as constraints or model coefficients.
 Once a BN is created and calibrated, it can be used to make predictions
 by selecting bins corresponding to input values and noting the changes
 in probability distributions of the output nodes.
 A user can also interact with a BN in an inverse-modeling context by specifying
 specific outputs and evaluating which are the most likely combinations
 of inputs that would result in the selected outputs.
 
\end_layout

\begin_layout Standard
An example of a BN created and visualized using Netica is presented in figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:exampleBN"

\end_inset

.
 In this example, each input node is connected to each output node and the
 
\begin_inset Quotes eld
\end_inset

Recharge
\begin_inset Quotes erd
\end_inset

 node serves as an intermediate node between 
\begin_inset Quotes eld
\end_inset

Transmissivity
\begin_inset Quotes erd
\end_inset

 and all the output nodes.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/net_Example.pdf
	lyxscale 35
	scale 35

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:exampleBN"

\end_inset

Example groundwater application of a Netica BN showing input (outlined in
 a red box) and output (outlined in a blue box) nodes, edges (black lines)
 and, in this case, a single intermediate node (recharge).
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Calculations are made using the BN based on conditional probabilities using
 Bayes’ Theorem 
\begin_inset Formula 
\begin{equation}
p\left(F_{i}|O_{j}\right)=\frac{p\left(O_{j}|F_{i}\right)p\left(F_{i}\right)}{p\left(O_{j}\right)}\label{eq:bayesthm}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $p\left(F_{i}|O_{j}\right)$
\end_inset

 is the posterior (updated) probability of a forecast 
\begin_inset Formula $\left(F_{i}\right)$
\end_inset

 given (conditional on) a set of observations 
\begin_inset Formula $\left(O_{j}\right)$
\end_inset

; 
\begin_inset Formula $p\left(O_{j}|F_{i}\right)$
\end_inset

 is the likelihood function, 
\begin_inset Formula $p\left(F_{i}\right)$
\end_inset

 is the prior probability of the forecast, and 
\begin_inset Formula $p\left(O_{j}\right)$
\end_inset

 is a normalizing constant.
 The posterior probability reflects an updating that is achieved by considering
 the entire chain of conditional probabilities of all bins connected to
 the node representing 
\begin_inset Formula $F_{i}$
\end_inset

.
 This 
\begin_inset Quotes eld
\end_inset

chain
\begin_inset Quotes erd
\end_inset

 is made up of all the nodes connected directly or through other nodes,
 as represented by edges.
 The likelihood function represents the probability that the observations
 
\begin_inset Formula $\left(O_{j}\right)$
\end_inset

 would be observed given that the forecast was perfectly known.
 This is a metric of the ability of the BN to function as a forecasting
 device and imperfections in such forecasts are a function of epistemic
 uncertainty.
 Epistemic uncertainty includes uncertainty due to model imperfection, data
 errors, data paucity, and other sources.
 The prior probability of the forecast, 
\begin_inset Formula $p\left(F_{i}\right)$
\end_inset

, is the probability of a forecast without the benefit of updated observations
 and the BN (or a process model or other experiment).
 
\begin_inset Formula $p\left(F_{i}\right)$
\end_inset

 may be estimated by using expert knowledge, or may be assumed relatively
 uninformative to make the entire process as objective as practical (similar
 to an ignorance prior 
\begin_inset CommandInset citation
LatexCommand citep
key "jaynesBook"

\end_inset

).
 A common prior often used in BNs is the division of the probability distributio
n represented by a node into bins of equal probability.
 This results in bins of equal probability or ‘‘belief’’ although it is
 not exactly an ignorance prior because the probability mass in each bin
 may differ due to variable bin widths.
 When the underlying problem responds to thresholds in the data, it is often
 important to incorporate these thresholds as bin boundaries when defining
 the structure of the BN.
 Similarly, in some cases, specific categories even within a continuous
 distribution have important meaning and should be incorporate into the
 node structure.
\end_layout

\begin_layout Standard
It is possible to evaluate the contribution to all uncertainty values calculated
 by the BN by expressing the uncertainty in the prior probabilities.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:exampleBN"

\end_inset

, the horizontal bars correspond to relative probabilities associated with
 bins outlined by the numbers listed to the left of them.
 These bars form a histogram and are referred to as ``belief bars.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
Once a system is cast in a BN, new observations of system state are applied
 and propagated through the BN using Bayes’ theorem such that all forecasts
 made in the model are contingent upon the specific observations.
 In other words, each forecast is associated with a specific configuration
 of system state.
 In our approach, observations are indicated by selecting the bin associated
 with that observation and forcing the probability of a value in the node
 to be 100%.
 This implies that observational uncertainty does not exceed the width of
 the specified bin (for continuous variables) or that the discrete or Boolean
 state is known perfectly.
 (It is straightforward to relax this assumption to consider inputs that
 are uncertain.) When this operation is performed, the Bayesian update propagates
 in each direction among nodes that are d-connected 
\begin_inset CommandInset citation
LatexCommand citep
key "JensenNielsen2001"

\end_inset

, updating the probabilities regardless of causal direction.
 Nodes are d-connected if there is an unblocked path of edges connecting
 them.
 In this way, correlations are expressed as well as causal responses.
 By selecting a suite of observations of state, the BN acts like a transfer
 function by providing an estimate of the forecast of interest and associated
 uncertainty.
 
\end_layout

\begin_layout Standard
A key piece of a priori information is the establishment of edges connecting
 the nodes.
 Edges should reflect a cascade of causality grounded in an understanding
 of the underlying process being modeled.
 If multiple processes from different models are to be linked, the selection
 of edge relationships defines the linkage.
 While machine learning can be used to teach a BN which parameters are connected
 to each other and to outputs, we adopt a Bayesian approach in which expert
 system understanding is used to specify these connections through the identific
ation of nodes and edges.
 In this way, the BN honors the physical conditions known by the modeler
 and these are incorporated as soft knowledge.
 It is possible to learn the structure of a BN from data 
\begin_inset CommandInset citation
LatexCommand citep
after "Chapter 7"
before "e.g. "
key "JensenNielsen2001"

\end_inset

 rather than expert understanding and in doing so, the resulting BN is more
 similar to an Artificial Neural Network 
\begin_inset CommandInset citation
LatexCommand citep
before "e.g. "
key "NeuralNetworks2006"

\end_inset

.
 In this work, the structure is always specified 
\emph on
a priori 
\emph default
through expert system understanding.
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:exampleBN"

\end_inset

, arrows on the edges indicate the direction of causal dependence.
 When all nodes are d-connected, the direction of the edge arrows serve
 no purpose in the calculations of conditional probabilities.
 However, in the context of d-separation, the direction of causality has
 important ramifications on the propagation of uncertainty from observations
 to forecasts and in either case, directionality of the edges helps users
 keep track of causal relationships when visually interpreting a BN.
\end_layout

\begin_layout Standard
The final step in calibrating a BN is parameter estimation.
 In this step, conditional probabilities are learned through enumeration
 or approximation.
 When computational conditions and problem size permit, a conditional probabilit
y table (CPT) can be created that directly enumerates the conditional probabilit
ies of all nodes in the BN.
 This becomes impractical rapidly, however, because the size of the CPT
 scales on the order of 
\begin_inset Formula $n\times d^{k+1}$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the number of nodes, 
\begin_inset Formula $d$
\end_inset

 is the number of bins, and 
\begin_inset Formula $k$
\end_inset

 is the number of parents for a node.
 In the case where full enumeration is impractical due to this rapid increase
 in computational expense with complexity, an iterative expectation-maximization
 (EM) algorithm is used 
\begin_inset CommandInset citation
LatexCommand citep
key "Dempster1977"

\end_inset

 to calculate approximate probabilities and maximum-likelihood values for
 the BN without full enumeration of the CPT.
 The EM algorithm iterates between estimating the maximum log likelihood
 of the function and finding the set of parameters resulting in that maximum
 log likelihood.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Section
CVNetica
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
CVNetica is a Python module that performs cross-validation and calculates
 other performance metrics on BNs created with the Netica software package.
 Netica is a commercial package with more power than open-source alternatives.
 However, CVNetica is open-source and freely available so requires no additional
 license.
 The APIs for Netica are described in 
\begin_inset CommandInset citation
LatexCommand citep
key "NeticaAPI"

\end_inset

 and are provided as a dynamic linked library (DLL) for Windows.
 Static libraries are also available for Macintosh and *Unix-based platforms,
 but to use them with Python, dynamic interface wrappers around the static
 functions would be necessary in addition to the Python function wrappers
 written in CVNetica.
 In this section we first describe the metrics and methods used to evaluate
 BNs and then describe the program structure of CVNetica.
\end_layout

\begin_layout Subsection
Metrics and Methods
\end_layout

\begin_layout Standard
The core functionality of CVNetica is based around the concept of using
 cross-validation 
\begin_inset CommandInset citation
LatexCommand citep
key "StatLearning2009,Marcot2012,FienenBNWRR"

\end_inset

 metrics to asses the quality of predictions made by a BN.
 An accurate assessment of BN predictions requires predictions to be made
 on data that were not included in the dataset used to calibrate the BN
 
\begin_inset CommandInset citation
LatexCommand citep
key "StatLearning2009"

\end_inset

.
 This can be accomplished either by reserving a single subset of available
 data for evaluating predictions or by using cross validation.
 In 
\emph on

\begin_inset Formula $k-$
\end_inset


\emph default
fold cross validation used in this work, the calibration dataset is, randomly
 without replacement, divided into 
\begin_inset Formula $k$
\end_inset

 folds or partitions where 
\begin_inset Formula $k$
\end_inset

 typically is between 2 and 10.
 For each fold, the BN is trained using the dataset without the data in
 the fold, then the BN is used to make predictions on the left-out data.
 In this way, performance of the BN is evaluated on data not used in calibration
 to simulate performance in true future prediction.
 This approach was selected both to make the most of data available (e.g.
 every data point is used at some stage in both calibration and prediction)
 and to smooth over any bias that may result in the random selection of
 a single withheld subset of the data.
 As a result, when metrics are reported over the 
\begin_inset Formula $k-$
\end_inset

folds, the standard deviation of the metrics are also reported to acknowledge
 the variability in selection among the fold samples.
\end_layout

\begin_layout Standard
Several performance metrics can be used for this purpose, as discussed in
 
\begin_inset CommandInset citation
LatexCommand citet
key "Netica"

\end_inset

, 
\begin_inset CommandInset citation
LatexCommand citet
key "plant2011a"

\end_inset

, and 
\begin_inset CommandInset citation
LatexCommand citet
key "FienenBNWRR"

\end_inset

.
 In this work, we focus on skill.
 Skill is evaluated based on a single representative expected value for
 each prediction as compared to a single observation corresponding to the
 same input values used to make the prediction.
 In the examples considered in this work, a single representative prediction
 is often considered as the key output of the BN in practice.
 Additional metrics take posterior variance into more explicit consideration.
 Skill is defined as 
\begin_inset Formula 
\begin{equation}
sk=\left[1-\frac{\sigma_{e}^{2}}{\sigma_{o}^{2}}\right]\label{skill}
\end{equation}

\end_inset

where 
\begin_inset Formula $\sigma_{e}^{2}$
\end_inset

 is the mean squared error between observations and transformed BN predictions,
 and 
\begin_inset Formula $\sigma_{o}^{2}$
\end_inset

 is the variance of the observations 
\begin_inset CommandInset citation
LatexCommand citep
key "Gutierrez2011,plant2011a,Weigend1994"

\end_inset

.
 Because the BN predictions are actually in the form of probabilities of
 the output variable falling within discrete ranges (bins), these probabilities
 are transformed to the expected values.
 Expected values may be computed either as mean or most likely (ML).
 The mean values are computed as the product of the bin centers and the
 probability in each bin, consistent with a typical expectation operation.
 For ML values, the value corresponding to the center of the bin with the
 highest predicted probability is reported.
 Additionally, since the transformation to an expected value does not capture
 all of the probabilistic information of the BN prediction, the expected
 values are fit to the observation via weighted linear regression, where
 the weights are set equal to the the reciprocal of the variance of the
 BN prediction in order to give more weight to more confident predictions
 and 
\begin_inset Formula $\sigma_{e}^{2}$
\end_inset

is computed as the mean-square regression error.
 A skill value of unity indicates perfect correspondence, a value of zero
 indicates substantial discrepancy between BN predictions and observations.
 Skill is closely related to the Nash-Sutcliffe Model Efficiency metric
 
\begin_inset CommandInset citation
LatexCommand citep
key "NS1970"

\end_inset

.
\end_layout

\begin_layout Standard
In addition to skill, CVNetica also reports log loss, error rate, experience,
 quadratic loss (Brier score), mutual information (entropy), and variance
 reduction (sensitivity) all of which are described by 
\begin_inset CommandInset citation
LatexCommand citet
key "Netica,NeticaAPI"

\end_inset

.
 These additional metrics in varying degree consider the probabilistic character
istics of the predictions made by a BN.
 
\begin_inset CommandInset citation
LatexCommand citet
key "EMSPosition2013"

\end_inset

 provide some guidance as to which metrics and methods are most appropriate
 for characterizing model performance in various contexts.
\end_layout

\begin_layout Standard
By evaluating skill over both the calibration data sets and prediction data
 sets, the value of a BN as a descriptive or predictive tool can be evaluated.
 As BN complexity increases, so does the calibration 
\begin_inset Formula $sk$
\end_inset

 and with sufficient complexity, calibration 
\begin_inset Formula $sk$
\end_inset

 approaches unity (perfection) in the extreme case with a bin for every
 unique value.
 However, greater descriptive value in a BN comes at a cost in predictive
 value.
 This is the classic condition of overfitting as cast in the context of
 information theory by 
\begin_inset CommandInset citation
LatexCommand citet
key "FienenBNWRR"

\end_inset

.
 Cross validation allows users to assess the cost in predictive value imposed
 by the addition of complexity through more bins, additional nodes, or edges.
 
\end_layout

\begin_layout Standard
One way to systematically evaluate BN complexity is to adjust the number
 of bins for each node with more bins meaning a greater level of complexity.
 CVNetica has the capability to make this type of analysis efficient by
 allowing the user to specify an original BN and a configuration of bins
 for each node and the examples in this work are evaluated using bin configurati
on to represent complexity.
 CVNetica then builds a new BN with the requested number of bins and assigns
 equiprobable prior distributions for each bin.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "FienenBNWRR"

\end_inset

 the number of bins was assumed the same for each node.
 Using CVNetica the number of bins in each node can be varied independently
 to allow for exploration of various assumptions of complexity.
 The user can also establish scenarios manually varying the number and nature
 of edges connecting nodes and even the number of nodes themselves.
 A group of these scenarios is defined by CVNetica as a ``set.
\begin_inset Quotes erd
\end_inset

 Each set can be evaluated as a batch and then tabulated.
 Graphical results are generated of performance metrics across the sets.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Program structure
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are three levels at which CVNetica performs.
 At the highest level, a script in 
\begin_inset Formula $\mathtt{CV\_driver.py}$
\end_inset

 performs the cross validation protocol described below.
 This script is driven by an XML-based configuration file and should generally
 require minimal editing, save for identifying the configuration file to
 use in the 
\begin_inset Formula $\mathtt{parfile}$
\end_inset

 variable name.
 At an intermediate level, 
\begin_inset Formula $\mathtt{pythonNetica.py}$
\end_inset

 provides the 
\begin_inset Formula $\mathtt{pynetica}$
\end_inset

 class that combines several Netica functions for tasks such as starting
 a Netica environment, rebinning nodes, and other intermediate level tasks.
 The intermediate level depends on the lowest level, 
\begin_inset Formula $\mathtt{pythonNeticaTools.py}$
\end_inset

 , which provides the 
\begin_inset Formula $\mathtt{pyneticaTools}$
\end_inset

 class that interacts with the Netica DLL via wrappers around many essential
 Netica functions.
 Examples of how these methods work are discussed in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Accessing-the-DLL"

\end_inset

.
 Call graphs in the supplemental online material also illustrate the relationshi
ps among these levels.
\end_layout

\begin_layout Standard
The 
\begin_inset Formula $\mathtt{CV\_driver.py}$
\end_inset

 script drives a cross-validation exercise on an existing BN developed using
 Netica specified in the XML based configuration file (Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:XMLinput"

\end_inset

).
 The script reads in the existing BN, imposes the specified alterations,
 runs the resulting BN, and gathers and summarizes the results.
 If no rebinning is requested (
\begin_inset Formula $\mathtt{<rebin\_flag>False</rebin\_flag>}$
\end_inset

), the BN specified in the 
\begin_inset Formula $\mathtt{baseNET}$
\end_inset

 element is used for analysis along with the casefile identified by the
 
\begin_inset Formula $\mathtt{baseCAS}$
\end_inset

 element and metrics of performance.
 If the 
\begin_inset Formula $\mathtt{rebin\_flag}$
\end_inset

 element is True, then the nodes from the BN identified in the 
\begin_inset Formula $\mathtt{originalNET}$
\end_inset

 element are rediscretized using the information on rebinning provided at
 the end of the input file.
 For each node listed, if 
\begin_inset Formula $\mathtt{numbins>0}$
\end_inset

 the node is discretized into bins 
\begin_inset Formula $\mathtt{numbins}$
\end_inset

 bins of equal probability.
 In the special case where 
\begin_inset Formula $\mathtt{numbins=0}$
\end_inset

, the node is not rediscretized but it is used either as input or response
 as described by the 
\begin_inset Formula $\mathtt{input}$
\end_inset

 and 
\begin_inset Formula $\mathtt{response}$
\end_inset

 elements above.
 This special case allows for other discretization strategies (such as threshold
s) to be implemented for nodes that are to be treated as input or response
 nodes but without equiprobable discretization.
 Nodes that are not identified as either input or response should not have
 
\begin_inset Formula $\mathtt{node}$
\end_inset

 elements provided and are unaltered by CVNetica in the analysis.
\end_layout

\begin_layout Standard
If the 
\begin_inset Formula $\mathtt{CVflag}$
\end_inset

 element is False, only a single run using all the data in the 
\begin_inset Formula $\mathtt{baseCAS}$
\end_inset

 file and the BN identified in the 
\begin_inset Formula $\mathtt{baseNET}$
\end_inset

 is performed and metrics are calculated.
 The predictions for each configuration of input are recorded in a compressed
 Python pickle file.
\end_layout

\begin_layout Standard
If the 
\begin_inset Formula $\mathtt{CVflag}$
\end_inset

 element is True, then k-fold cross validation is performed using the number
 of folds indicated in the 
\begin_inset Formula $\mathtt{numfolds}$
\end_inset

 element.
 For each fold, 
\begin_inset Formula $\frac{n}{k}$
\end_inset

 (where 
\begin_inset Formula $n$
\end_inset

 is the total number of data points and 
\begin_inset Formula $k$
\end_inset

 is the number of folds) data points are separated from the rest of the
 data points to be left out of the calibration, selecting from a randomized
 list such that each fold samples across the training set to span spatial
 or temporal trends or patterns.
 The BN is then retrained on the 
\begin_inset Formula $n-\frac{n}{k}$
\end_inset

 retained data and metrics of performance are calculated for both the left
 out data (referred to as ``validation
\begin_inset Quotes erd
\end_inset

) and the training data (referred to as ``calibration
\begin_inset Quotes erd
\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/imputfileexample.pdf
	scale 130

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:XMLinput"

\end_inset

Example XML configuration file for defining problem parameters.
 Blue text identifies syntax of element names, green text indicates comments
 in the file, and bold black text indicates element values.
 In the special case of the 
\begin_inset Formula $\mathtt{node}$
\end_inset

 element, an attribute (numbins) is indicated in red text.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Working with Ctypes
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The Netica software provides APIs for accessing and using the functions
 within it.
 Several versions of these APIs are available as precompiled libraries.
 To interface with Python, the C programming language APIs can be interfaced
 using the 
\family typewriter
ctypes
\family default
 module which is built-in to Python 2.5+.
 The 
\family typewriter
ctypes
\family default
 module enables the use of functions from a dynamic library of C code (a
 DLL on Windows) in the Python environment.
 In addition to making the functions accessible, some translation of variables
 is required--for example, C often refers to data using pointers whereas
 Python does not explicitly do so.
 C functions often return pointers to memory space of the resulting arrays
 so 
\family typewriter
ctypes
\family default
 must be used to read the correct amount of data from memory to populate
 an array for further use in Python.
 
\end_layout

\begin_layout Standard
CVNetica provides Python functions wrapped around Netica C functions and
 helper functions to translate data to and from the Python environment.
 In the remainder of this section, the main aspects of interfacing with
 the Netica APIs are discussed in general terms.
 These examples use code snippets from the CVNetica codebase.
 Further documentation about 
\family typewriter
ctypes
\family default
 is available from the official documentation (http://docs.python.org/2/library/ct
ypes.html).
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sub:Accessing-the-DLL"

\end_inset

Accessing the DLL
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The first task when accessing the Netica DLL is to make the functions available
 to Python by assigning the DLL to an object.
 Note that the filename is not in quotes, nor is the 
\family typewriter
.dll
\family default
 extension required.
 The 
\family typewriter
ctypes
\family default
 module is imported as 
\family typewriter
ct
\family default
 so in future code descriptions, 
\family typewriter
ct.<>
\family default
 implies a method or property from 
\family typewriter
ctypes
\family default
.
 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

import ctypes as ct
\end_layout

\begin_layout Plain Layout

self.n = ct.windll.Netica
\end_layout

\end_inset


\end_layout

\begin_layout Standard
After this, 
\family typewriter
self.n
\family default
 is an object with all of the Netica API functions available.
 To call a function from the DLL, the function name is dereferenced from
 
\family typewriter
self.n
\family default
 and in CVNetica, a wrapper function is created as an interface to the Netica
 function.
 In the following example, the Netica function to be called is 
\family typewriter
EnterNodeValue_bn
\family default
.
 This function takes two arguments as indicated in the function definition
 by Netica: 
\family typewriter
void EnterNodeValue_bn (node_bn* node, double value)
\family default
 
\begin_inset CommandInset citation
LatexCommand citep
key "NeticaAPI"

\end_inset

.
 The two arguments are of the custom C type defined by Netica as 
\family typewriter
node_bn* node
\family default
 and a double-precision float 
\family typewriter
double value
\family default
.
 A wrapper around this function must make type conversions as appropriate.
 The CVNetica variable 
\family typewriter
cnode
\family default
 is of the custom C type 
\family typewriter
node_bn*
\family default
, and it was returned by a Netica API function, so it is already of the
 required type (in this case a pointer) was returned by a Netica function,
 so it is already of the type required.
 However, the CVNetica variable 
\family typewriter
cval
\family default
 is a Python float and must be converted to a C double using a 
\family typewriter
ctypes
\family default
 conversion.
 
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

def EnterNodeValue(self,cnode,cval):
\end_layout

\begin_layout Plain Layout

	self.n.EnterNodeValue_bn(cnode,ct.c_double(cval))         
\end_layout

\begin_layout Plain Layout

	self.chkerr()
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\family typewriter
chkerr
\family default
 method polls the Netica DLL for current error status and, if an error is
 encountered, kills CVNetica and displays the error from Netica to standard
 error.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Exchanging information with the Netica DLL
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The functions in Netica can accept a variety of argument types.
 In the 
\begin_inset Formula $\mathtt{pyneticaTools}$
\end_inset

 class, methods that function as wrappers around Netica functions are written.
 The names are the same as the Netica functions with the 
\begin_inset Formula $\mathtt{\_bn}$
\end_inset

, 
\begin_inset Formula $\mathtt{\_cs}$
\end_inset

, and 
\begin_inset Formula $\mathtt{\_ns}$
\end_inset

 suffixes removed.
 This class is not specific to cross validation applications and is meant
 to also serve as a starting point for other applications in which Netica
 functions must be used in Python.
\end_layout

\begin_layout Standard
The easiest argument type is a pointer to an object returned by another
 Netica function.
 In this case, a Python variable represents the pointer--just a memory address--
so no conversion is necessary.
 For single Python floats and ints, the conversions are 
\family typewriter
ct.c_double(cval)
\family default
 and 
\family typewriter
ct.c_int(cval)
\family default
, respectively, where 
\family typewriter
cval
\family default
 is the Python variable.
 
\end_layout

\begin_layout Standard
Some Netica functions return a double value but also write another result
 to memory at a location indicated by a pointer passed to the function.
 An example is 
\family typewriter
GetNodeExpectedValue_bn
\family default
.
 The structure of this function in C is
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

double GetNodeExpectedValue_bn (node_bn* node, 
\end_layout

\begin_layout Plain Layout

	double* std_dev, double* x3, double* x4)
\end_layout

\end_inset


\family default
where the value returned by the function is the expected value (double precision
) of the node identified by 
\family typewriter
node_bn*
\family default
, the standard deviation is written to the memory location identified by
 the pointer 
\family typewriter
double* std_dev
\family default
, and 
\begin_inset Formula $\mathtt{x3}$
\end_inset

 and 
\begin_inset Formula $\mathtt{x4}$
\end_inset

 are NULL pointers reserved for future implementation.
 To collect the main returned value of the function, we must set 
\begin_inset Formula $\mathtt{restype}$
\end_inset

 of the function--accomplished through making an alias temporary function--and
 accepting the value as normally with a function.
 To make use of the returned second value in Python--the value written to
 a memory location identified by a pointer--we must pass a 
\begin_inset Formula $\mathtt{double}$
\end_inset

 variable by reference (in other words, a pointer to the 
\begin_inset Formula $\mathtt{double}$
\end_inset

).
 The Python wrapper for 
\family typewriter
GetNodeExpectedValue_bn
\family default
 illustrates this process 
\end_layout

\begin_layout Standard

\family typewriter
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

def GetNodeExpectedValue(self,cnode): 
\end_layout

\begin_layout Plain Layout

	std_dev = ct.c_double() # use ctypes conversion to declare 
\end_layout

\begin_layout Plain Layout

				# std_dev a double precision variable
\end_layout

\begin_layout Plain Layout

	tmpNeticaFun = self.n.GetNodeExpectedValue_bn # make a 
\end_layout

\begin_layout Plain Layout

					# temporary function
\end_layout

\begin_layout Plain Layout

	tmpNeticaFun.restype=ct.c_double # use restype to define
\end_layout

\begin_layout Plain Layout

				# that a double variable is returned 
\end_layout

\begin_layout Plain Layout

				# by the function 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	# note that in the call to the temporary function
\end_layout

\begin_layout Plain Layout

	# the returned value (expected_val) is set by the 
\end_layout

\begin_layout Plain Layout

	# function, and std_dev is set in memory and dereferenced
\end_layout

\begin_layout Plain Layout

	# with the method .value
\end_layout

\begin_layout Plain Layout

	expected_val = tmpNeticaFun(cnode,ct.byref(std_dev), 
\end_layout

\begin_layout Plain Layout

                                None,None) 
\end_layout

\begin_layout Plain Layout

	self.chkerr() 
\end_layout

\begin_layout Plain Layout

	return expected_val, std_dev.value
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Some Netica functions return either a character array or a numerical array.
 In both cases, the C code in Netica returns a pointer to the data.
 The Python code must, then, read a specified amount of data from that pointer
 location.
 Unlike pure Python, it is possible to read off the end of the information
 starting at the pointer location, so we must also specify the number of
 values to read from the memory location.
 Helper functions in 
\begin_inset Formula $\mathtt{cthelper.py}$
\end_inset

 read the character pointers, and single and double precision pointers.
 An example of this being used in CVNetica is in the 
\begin_inset Formula $\mathtt{ReadNodeInfo}$
\end_inset

 method of the 
\begin_inset Formula $\mathtt{pyneticaTools}$
\end_inset

 class.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Example Applications
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The CVNetica code was applied to two different applications to evaluate
 predictive performance and guide the appropriate level of complexity for
 BN design.
 The two applications are (1) a data-driven prediction of ocean wave evolution
 and (2) a model emulation using a BN to make predictions trained on results
 of a groundwater flow model shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:exampleBN"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Data driven ocean waves
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Weather forecasting and modeling has achieved sufficiently high accuracy
 that it is possible to replace observations with models if models are initializ
ed well and have good boundary condition data.
 However, weather forecasts are not routinely available for periods extending
 more than a few days ahead, and they become less accurate.
 We would like to allow the climatological prior information to inform predictio
ns when observations or forecasts are not available or are uncertain.
 As an example, we would like to predict wave height just offshore of the
 coast where there are not persistent observations.
 This could be done with laborious Monte Carlo simulations using models
 and previous climatology for model initialization and boundary-condition
 forcing.
 Or, we could use extant model output or observations to learn both the
 sensitivity of a specific prediction to changes in boundary conditions
 and include uncertainty in this sensitivity (the joint correlation) as
 well as uncertainty in the boundary conditions.
 This approach has been implemented before using Bayesian networks 
\begin_inset CommandInset citation
LatexCommand citep
key "plant2011a,plant2011b"

\end_inset

, but the fidelity of the resulting BN models was not examined in detail,
 other than to note that to maximize the consistency of the BN predictions
 with new observations, the BN inputs should include input parameter-value
 or data uncertainties.
 Were these BN models over-fit to the data?
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/bermPy_v4.pdf
	scale 40

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:BNocean"

\end_inset

BN for the wave height prediction model.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We explore the level of overfitting with a simplified ocean-wave prediction
 model based on a BN.
 The specific BN, illustrated in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:BNocean"

\end_inset

, has been used to drive subsequent predictions of morphologic evolution
 of a man-made sand berm constructed near the Chandeleur Islands 
\begin_inset CommandInset citation
LatexCommand citep
key "Plantetal2014"

\end_inset

.
 Here, we simplify the original model, which included information from two
 wave buoys, one tide gage, and Monte Carlo simulation of a wave-runup model
 
\begin_inset CommandInset citation
LatexCommand citep
key "Stockdon2006"

\end_inset

 and the full dataset consisted of 50,385 entries.
 The first two columns of network nodes (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:BNocean"

\end_inset

) correspond to observations from an offshore buoy (NOAA 42040) collected
 from 1996 to 2011 that is located far offshore of the location where prediction
s are needed.
 The offshore variables are wind speed and direction and wave height, period,
 and direction.
 The third column has just one variable--wave height--that was observed
 at an onshore buoy between 2000 and 2008.
 The onshore buoy was subsequently lost.
 The BN describing the prior probabilities of each variable and the conditional
 probabilities among variables was designed to resolve boundary conditions
 at the offshore location and the prediction at the nearshore location accuratel
y enough to support the morphologic evolution application.
 While this BN has very good hindcast skill (about 0.8), it is not clear
 that it has equally good forecast skill and whether fewer probability bins
 could be retained to give a skillful prediction with better numerical efficienc
y.
 
\end_layout

\begin_layout Standard
Our calibration/validation skill analysis was applied to this BN by varying
 the number of bins in all variables except for the wave heights.
 We chose to resolve wave heights consistently with the original model to
 ensure that probability predictions spanned a wide range conditions, rather
 than focusing on the most probable but extremely low wave-height range
 that was most common (i.e., 1-3 m).
 The number of bins ranged from 2 to 10 for the remaining variables.
 The calibration (i.e., hindcast) skill increased for all choices of bin numbers
 (figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:CVocean"

\end_inset

).
 However, the validation skill, averaged over 5 folds, reached its peak
 value at 4 bins and then decreased dramatically after 6 bins.
 It is likely that the optimal bin resolution varied for each variable type
 (wind speed, wind and wave directions, wave period) and this may explain
 the flattening of the validation curve between 4 and 6 bins, as it is possible
 that increasing bin resolution was advantageous, adding necessary resolution,
 for some variables but disadvantageous for others.
 The rapid decline after 6 bins suggests that none of the variables needed
 to be better resolved past this point.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/berm_mean_skillMean.pdf
	scale 50

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:CVocean"

\end_inset

Calibration (descriptive) and Validation (predictive) skill values for various
 arrangements of bins on the wave prediction BN.
 For sets identified by a single number, that number of bins was used to
 discretize all nodes.
 The color shading indicates an approximation of the 95% credible interval
 based on adding and subtracting 
\begin_inset Formula $2\sigma$
\end_inset

 to and from the median value of each metric over the 10 folds evaluated.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Model emulation source of groundwater to wells
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the Great Lakes Region of the United States, understanding the interactions
 between groundwater and surface water are important inputs to ecological
 management interests.
 Specifically, as pumping wells are installed, the baseflow in streams can
 be reduced and these reductions can affect fish habitat and associated
 societal and economic concerns 
\begin_inset CommandInset citation
LatexCommand citep
key "Ruswick2010,BarlowLeakeCirc,WatsonReeves2014"

\end_inset

.
 An efficient method to determine the source of water to wells has the potential
 to improve management in the region by quickly screening proposed wells.
 If the source of water emanating from surface water (either through diversion
 or depletion) reaches a management threshold, then further management actions
 may be triggered.
 Using a numerical groundwater model, managers could conceivably explicitly
 assess the impact of each proposed well location.
 But computational run times and technical background may be prohibitive
 for that task.
 A more efficient option is model emulation as performed on a groundwater
 model by 
\begin_inset CommandInset citation
LatexCommand citet
key "FienenBNWRR"

\end_inset

.
\end_layout

\begin_layout Standard
In this case--using MODFLOW-USG 
\begin_inset CommandInset citation
LatexCommand citep
key "MFUSG"

\end_inset

--extraction wells were simulated on multiple staggered grids at sufficient
 distances that they would not interact in individual model runs.
 A base case was also simulated without extraction wells and, through superposit
ion, the sources of water to the wells was evaluated and mappable characteristic
s of each well location were used to create the BN in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:exampleBN"

\end_inset

.
 In all, 4,905 wells were simulated, requiring nine model runs.
 For further discussion of the model used in this work see 
\begin_inset CommandInset citation
LatexCommand citet
key "Feinstein2013"

\end_inset

.
\end_layout

\begin_layout Standard
An important question--similar to that evaluated by 
\begin_inset CommandInset citation
LatexCommand citet
key "FienenBNWRR"

\end_inset

--is what level of complexity provides the best tradeoff between descriptive
 and predictive power of the BN? Using CVNetica, it was possible to quickly
 evaluate 
\begin_inset Formula $k-$
\end_inset

fold cross validation for a variety of combinations of bins in the node
 arrangement depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:exampleBN"

\end_inset

.
 For the most important response variable--SW_SRC, which is surface water
 source--Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:CVglacial"

\end_inset

 depicts the change in both calibration and validation performance for 10-fold
 cross validation performed over an increasingly complex set of bin configuratio
ns.
 While increasing complexity (e.g., number of bins per node) monotonically
 improves calibration (description) over the training set, the skill improves
 at first for validation (prediction) but then degrades dramatically after
 four bins on the input nodes.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:CVglacial"

\end_inset

, sets identified by a single value indicate the number of bins used to
 discretize each node.
 When a second number is present, the first number indicates bins used for
 the input nodes while the second indicates bins used for the output nodes.
 Little degradation and possibly a slight improvement in validation skill
 is seen with increasing output bins for a given complexity of input.
 This highlights that the main fitting of the BN takes place with respect
 to input while output complexity is more a matter of convenience than a
 source of real BN complexity.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/glacial_CV.pdf
	scale 50

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:CVglacial"

\end_inset

Calibration (descriptive) and Validation (predictive) skill values for various
 arrangements of bins on the glacial aquifer BN.
 For sets identified by a single number, that number of bins was used to
 discretize all nodes.
 For sets identified by two numbers separated by an underscore, the first
 and second numbers indicate the number of bins the input and output nodes,
 respectively, were discretized.
 The color shading indicates an approximation of the 95% credible interval
 based on adding and subtracting 
\begin_inset Formula $2\sigma$
\end_inset

 to and from the median value of each metric over the 10 folds evaluated.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion and Conclusions
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
CVNetica is an open-source Python module using the 
\family typewriter
ctypes
\family default
 module to drive APIs for the Netica BN software.
 The purpose is to implement cross-validation techniques for evaluating
 descriptive (calibration) versus predictive (validation) performance of
 BNs.
 
\end_layout

\begin_layout Standard
We show that CVNetica provides an objective method for determining when
 a BN is being overfit to the data.
 And, it appears that overfitting is likely when the BN is designed to resolve
 physical processes, either observed or modeled.
 In the cases presented here, the BN design was guided by distribution of
 the priors of each variable as well as by the intended application of the
 BN predictions.
 For instance, in the ocean wave example, the intended application focused
 on resolving large storm events that were likely to cause erosion.
 This guided a choice of fairly high resolution of the input data at the
 offshore buoy.
 While physically consistent, a price needed to be paid for the over-resolved
 output in order to achieve true predictive skill.
 That price was to greatly reduce the resolution of the input variables
 from as many as 12 bins to no more than 6 bins.
 While a price is paid in terms of input detail, there is a numerical as
 well as statistical benefit to reducing the bin resolution.
 For instance, the original ocean wave BN maintained over a million possible
 combinations of inputs and outputs scenarios within its conditional probability
 tables while the optimal 4-bin BN maintained 44 times fewer scenarios,
 reduced memory requirements, and had increased training and prediction
 speeds.
 For instance, the CV processing took 25 minutes for the 4-bin net compared
 to over 8 hours for the original net (a factor of 20 difference).
 
\end_layout

\begin_layout Standard
In the model emulation case, fewer input bins were supported while maintaining
 good predictive power.
 Four input bins resulted in good performance while a degradation of predictive
 skill started with five input bins.
 Predictive skill was relatively consistent with respect to output bins
 between 5 and 10 for a given set of input bins.
 This allows a resource manager to convey outcomes with some flexibility
 beyond the level of complexity supported by the data on the input side.
\end_layout

\begin_layout Standard
CVNetica is available for download at (http://mnfienen-usgs.github.io/CVNetica)
 and the authors welcome proposed contributions to code development going
 forward.
 These diagnostics and others have the potential to improve the validity
 of BNs used for prediction in natural resources and other applications.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{linenumbers}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Acknowledgements
\end_layout

\begin_layout Standard
This work was funded by the USGS Groundwater Resources Program and the USGS
 Coastal and Marine Geology Program.
 We are deeply grateful to Steven Mascaro for his initial PyNetica.py code
 which he kindly shared as a starting point for this work.
 We also thank Howard Reeves, USGS, Tony Jakeman, Editor-in-Chief, and two
 anonymous reviewers for insightful comments that improved this manuscript.
\end_layout

\begin_layout Section
Disclaimer
\end_layout

\begin_layout Standard
Any use of trade, product, or firm names is for descriptive purposes only
 and does not imply endorsement by the U.S.
 Government.
 
\end_layout

\begin_layout Section
References
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "GW"
options "elsarticle-num-names"

\end_inset


\end_layout

\end_body
\end_document
